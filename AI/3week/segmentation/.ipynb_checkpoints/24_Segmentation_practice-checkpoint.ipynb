{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. FCN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqkKav8O6Fh1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os.path as osp\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip Kitti.zip -d ./data/Kitti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일을 제대로 다운로드 하였고 읽을 수 있는지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "error",
     "timestamp": 1636096005436,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "TwA5VAjp6Fh7",
    "outputId": "24ad80d0-4595-45b5-b8c4-c7b3da462966",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgsets_file = osp.join('./data/Kitti', '{}.txt'.format('train'))\n",
    "for line in open(imgsets_file):\n",
    "    line = line.strip()\n",
    "    print(line)\n",
    "    line = line.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEEnAaqx6Fh8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KITTIdataset(torch.utils.data.Dataset):\n",
    "    class_names = np.array(['background', 'road'])\n",
    "\n",
    "    def __init__(self, root, transform, split='train'):\n",
    "        self.root = ## implement code here\n",
    "        self.split = ## implement code here\n",
    "        self.transform = ## implement code here\n",
    "\n",
    "        self.image_path = []\n",
    "        self.ys = []\n",
    "        \n",
    "        imgsets_file = osp.join(root, '{}.txt'.format(split))\n",
    "        for did in open(imgsets_file):\n",
    "            did = did.strip()\n",
    "            did = did.split()\n",
    "            img_file = osp.join(root, 'data_road/{}'.format(did[0]))\n",
    "            lbl_file = osp.join(root, 'data_road/{}'.format(did[1]))\n",
    "            self.image_path.append(img_file)\n",
    "            self.ys.append(lbl_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image\n",
    "        img_file = ## implement code here\n",
    "        img = ## implement code here\n",
    "        \n",
    "        # load label\n",
    "        lbl_file = ## implement code here\n",
    "        lbl = ## implement code here\n",
    "        lbl = ## implement code here\n",
    "        lbl[lbl == 255] = 1 # 0 is black 255 is white\n",
    "        \n",
    "        return self.transform(img), torch.from_numpy(lbl).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "train_dataset = ## implement code here\n",
    "val_dataset = ## implement code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset), len(val_dataset))\n",
    "print(len(train_dataset[0]))\n",
    "print(train_dataset[0][0].shape, train_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = ## implement code here (use torch.utils.data.DataLoader, batch_size=1)\n",
    "val_loader = ## implement code here (use torch.utils.data.DataLoader, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation matric (mIoU)\n",
    "\n",
    "The evaluation matric code is given.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) +\n",
    "        label_pred[mask], minlength=n_class**2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "def compute_mean_iou(label_trues, label_preds, n_class):\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iou = np.nanmean(iu)\n",
    "    \n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOtKS8ll6Fh-"
   },
   "source": [
    "# Define the Network\n",
    "\n",
    "- FCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_class=3):\n",
    "        super().__init__()\n",
    "        self.loss = ## Apply cross entropy loss\n",
    "        self.num_class = ## implement code here\n",
    "    \n",
    "        #############################################################\n",
    "        # Structure of the FCN model\n",
    "        #\n",
    "        #   3 ->   64 2\n",
    "        #  64 ->  128 2\n",
    "        # 128 ->  256 3 conv->relu->conv->relu->conv->relu => Predict 3\n",
    "        # 256 ->  512 3 conv->relu->conv->relu->conv->relu => Predict 2\n",
    "        # 512 ->  512 3 conv->relu->conv->relu->conv->relu\n",
    "        # 512 -> 4096 2 conv->relu->conv->relu => Predict 1\n",
    "        \n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...)\n",
    "        #############################################################\n",
    "        \n",
    "        ## conv1\n",
    "        self.features1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding = 100), # the padding=100 is given for a reason! Other conv2d should all have padding=1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding =1),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        ## conv2\n",
    "        self.features2 = nn.Sequential() ## implement code here\n",
    "        \n",
    "        ## conv3\n",
    "        self.features3 = nn.Sequential() ## implement code here\n",
    "        \n",
    "        ## conv4\n",
    "        self.features4 = nn.Sequential() ## implement code here\n",
    "        \n",
    "        ## conv5\n",
    "        self.features5 = nn.Sequential() ## implement code here\n",
    "        \n",
    "        ## maxpool\n",
    "        self.maxpool = ## implement code here (stride=2, ceil_mode=True)\n",
    "        \n",
    "        #4096->4096->num_class\n",
    "        self.classifier = nn.Sequential() # conv(kernel_size=7) - relu - dropout - conv(kernel_size=1) - relu - dropout - conv(kernel_size=1)\n",
    "        \n",
    "        ## upsampling transposed convolution (use nn.ConvTranspose2d, in&out channel:num_class)\n",
    "        self.upscore2 =  # kernel:4, stride:2\n",
    "        self.upscore4 =  # kernel:4, stride:2\n",
    "        self.upscore8 =  # kernel:16, stride:8\n",
    "        \n",
    "        self.score_pool4 = ## conv for Predict 2\n",
    "        self.score_pool3 = ## conv for Predict 3\n",
    "\n",
    "        self.softmax = ## implement code here\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #################################\n",
    "        ## implement code here\n",
    "        ## Exercise 1에 나온 모델 구조 그림을 참고하세요.\n",
    "\n",
    "        x1 = ## implement code here\n",
    "        pool1 = ## implement code here\n",
    "\n",
    "        x2 = ## implement code here\n",
    "        pool2 = ## implement code here\n",
    "\n",
    "        x3 = ## implement code here\n",
    "        pool3 = ## implement code here\n",
    "\n",
    "        x4 = ## implement code here\n",
    "        pool4 = ## implement code here\n",
    "\n",
    "        x5 = ## implement code here\n",
    "        pool5 = ## implement code here\n",
    "\n",
    "        predict1 = ## implement code here\n",
    "    \n",
    "        deconv1 = ## implement code here\n",
    "        predict2 = ## implement code here\n",
    "        predict2 = predict2[:, :, 5:5 + deconv1.size()[2], 5:5 + deconv1.size()[3]] # 사이즈 조절을 위함\n",
    "        add1 = # use torch.add() to add two feature maps\n",
    "\n",
    "        deconv2 = ## implement code here\n",
    "        predict3 = ## implement code here\n",
    "        predict3 = predict3[:, :, 9:9 + deconv2.size()[2], 9:9 + deconv2.size()[3]] # 사이즈 조절을 위함\n",
    "        add2 = # use torch.add() to add two feature maps\n",
    "\n",
    "\n",
    "        deconv3 = ## implement code here\n",
    "        deconv3 = deconv3[:, :, 33:33 + x.size()[2], 33:33 + x.size()[3]] # 사이즈 조절을 위함\n",
    "        \n",
    "        out = ## implement code here\n",
    "        ##################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ## define model (num_class:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ## use cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ## use Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 2\n",
    "best_iou = 0\n",
    "num_class = len(train_loader.dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    \" Training \"\n",
    "    ## implement code here # Training mode\n",
    "    print ('current epoch : %d'%(epoch))\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        ## implement code here # clears old gradients from the last step\n",
    "        \n",
    "        ## implement code here # output\n",
    "        \n",
    "        ## implement code here # loss\n",
    "        \n",
    "        ## implement code here # computes the derivative of the loss w.r.t. the parameters using backpropagation\n",
    "        \n",
    "        ## implement code here # causes the optimizer to take a step based on the gradients of the parameters\n",
    "        \n",
    "        if batch_idx % 20 ==0:\n",
    "            print ('batch : {}, loss : {}'.format(batch_idx, loss.item()))\n",
    "\n",
    "        \n",
    "    \" Validation \"\n",
    "    ## implement code here # Evaluation mode\n",
    "    val_loss = 0\n",
    "    metrics = []\n",
    "    with torch.no_grad(): # Gradient calculation X\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            score = model(data)\n",
    "\n",
    "            loss = criterion(score, target)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, lbl_pred = score.max(1)\n",
    "            lbl_pred = lbl_pred.cpu().numpy()  \n",
    "            lbl_true = target.cpu().numpy()\n",
    "\n",
    "            for lt, lp in zip(lbl_true, lbl_pred): # lbl_true, lbl_pred: [batch, h, w]\n",
    "                tmp = compute_mean_iou(lt, lp, num_class)\n",
    "                metrics.append(tmp)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    metrics = np.mean(metrics)\n",
    "    \n",
    "    print ('val loss : {}, mean_iou : {}'.format(val_loss, metrics))\n",
    "\n",
    "    # save model\n",
    "    if best_iou < metrics:\n",
    "        best_iou = metrics\n",
    "        print(\"Best model saved\")\n",
    "        torch.save(model.state_dict(), './model_best.pth')\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. U-Net implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_class=3):\n",
    "        super().__init__()\n",
    "        self.loss = ## implement code here ## apply cross entropy loss\n",
    "        self.num_class = ## implement code here\n",
    "\n",
    "        ##################################################\n",
    "        # 자유롭게 U-Net 구조를 만들어보세요.\n",
    "        # 24_Segmentation.pdf 파일의 Exercise 2. U-Net implementation 설명 figure를 참고하셔도 좋습니다.\n",
    "        # 간소한 버전으로 구현을 하고, 최대한 skip-connection을 사용하는 방법을 생각해보세요.\n",
    "        \n",
    "        # self.@@@ = nn.@@@@\n",
    "        # ...\n",
    "\n",
    "\n",
    "        ##################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##################################################\n",
    "        # 자유롭게 U-Net 구조를 만들어보세요.\n",
    "        # 24_Segmentation.pdf 파일의 Exercise 2. U-Net implementation 설명 figure를 참고하셔도 좋습니다.\n",
    "        # 간소한 버전으로 구현을 하고, 최대한 skip-connection을 사용하는 방법을 생각해보세요.\n",
    "        \n",
    "        # x1 = self.@@@(x)\n",
    "        # ...\n",
    "\n",
    "\n",
    "        ##################################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet 코드는 실행되지 않아도 좋습니다. U-Net 구조와 Skip-connection의 활용법을 배우는 것이 목표입니다.  \n",
    "(UNet 코드의 경우 데이터의 사이즈 문제 등으로 그대로 실행시 돌아가지 않을 가능성이 큽니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3. Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DeepLabV3\n",
    "model = deeplabv3_resnet101(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "image_path = './data/cat.jpg'  # 사용할 이미지 파일 경로\n",
    "image = Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image\n",
    "def crop_to_square(image):\n",
    "    h, w = image.size\n",
    "    new_size = min(h, w)\n",
    "    \n",
    "    # Calculate the cropping box\n",
    "    left = (w - new_size) / 2\n",
    "    top = (h - new_size) / 2\n",
    "    right = (w + new_size) / 2\n",
    "    bottom = (h + new_size) / 2\n",
    "    \n",
    "    # Crop the image\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    \n",
    "    return cropped_image\n",
    "\n",
    "preprocess = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "image = crop_to_square(image)\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # 배치 차원 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)['out'][0]\n",
    "output_predictions = output.argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(output_predictions.cpu().numpy(), cmap=\"jet\", alpha=0.5)\n",
    "plt.title(\"Segmentation Mask\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "seg_kitti_ans.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "cee20aa2885cadc07e824ce5082d40bca942426616eda434cad5578791d33ff8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

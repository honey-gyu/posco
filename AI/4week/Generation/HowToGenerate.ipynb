{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp3XPuaTu9jl"
   },
   "source": [
    "\n",
    "# How to generate text: using different decoding methods for language generation with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19S3UvyanLnB"
   },
   "source": [
    "### **GPT-2**\n",
    "\n",
    "#### ***Language Modeling***\n",
    "- ê¸°ì¡´ GPT-1ì€ pre-trainingê³¼ supervised fine-tuningì˜ ê²°í•©\n",
    "- GPT-2ëŠ” language modeling(token sequenceë¥¼ ì‚¬ìš©í•´ ë¬¸ì¥ì˜ ë¹„ì§€ë„ ë¶„í¬ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•)\n",
    "\n",
    "#### ***Various Training Dataset***\n",
    "- GPT-2ì˜ ê°€ì¥ í° ëª©ì ì€ Fine-tuning ì—†ì´ unsupervised pre-training ë§Œì„ Zero-shotìœ¼ë¡œ í†µí•´ ë‹¤ì–‘í•œ taskë¥¼ ì§„í–‰í•  ìˆ˜ ìˆëŠ” General Language Modelì„ ë§Œë“œëŠ” ê²ƒ\n",
    "- GPT-1ì€ news article, wikipediaë¥¼ ì£¼ë¡œ ì‚¬ìš©. GPT-2ëŠ” Web Text ì‚¬ìš©\n",
    "\n",
    "#### ***Byte Pair Encoding(BPE)***\n",
    "- subword ê¸°ë°˜ì˜ ì¸ì½”ë”© ë°©ë²•ìœ¼ë¡œ ë¬¸ì ë‹¨ìœ„ë¡œ ë‹¨ì–´ë¥¼ ë¶„í•´í•´ vocab ìƒì„±\n",
    "- OOV(Out Of Vocabulary) ë¬¸ì œ í•´ê²°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxLvv6UaPa33"
   },
   "source": [
    "### **Introduction**\n",
    "\n",
    "ìµœê·¼, ëŒ€ëŸ‰ì˜ weppage ë°ì´í„°ë¡œ í•™ìŠµ ëœ transformer-based language modelsì˜ ë“±ì¥ìœ¼ë¡œ open-ended language generationì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì¡ŒìŠµë‹ˆë‹¤.(ex. GPT-2)\n",
    "\n",
    "í–¥ìƒëœ tranformer architecureì™€ ëŒ€ëŸ‰ì˜ unsupervised trainig dataì™¸ì—ë„ ë” ë‚˜ì€ decoding methods ë˜í•œ ì¤‘ìš”í•œ ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "ë³¸ ìë£Œì—ì„œëŠ” ë‹¤ì–‘í•œ decoding strategiesì— ëŒ€í•œ ê°„ëµí•œ ê°œìš”ì™€  `transformers` libraryë¥¼ ì‚¬ìš©í•´ ì†ì‰½ê²Œ ì´ëŸ¬í•œ decoding strategiesë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "ëª¨ë“  ë°©ë²•ë“¤ì€ **auto-regressive** language generation([here](http://jalammar.github.io/illustrated-gpt2/) a refresher)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Zbm10Z96Cwq"
   },
   "source": [
    "![GPT-2 Auto regressive](http://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToPf40Ab6J8j"
   },
   "source": [
    "*Auto-regressive* language generationëŠ” word sequenceì˜ í™•ë¥ ì€ next wordì˜ conditional distributionìœ¼ë¡œ decomposeí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ê¸°ë³¸ ê°€ì •ì…ë‹ˆë‹¤.\n",
    "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n",
    "\n",
    "$W_0$ëŠ” initial *context* word sequenceë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. Word sequenceì˜ ê¸¸ì´ì¸ $T$ëŠ” $P(w_{t} | w_{1: t-1}, W_{0})$ìœ¼ë¡œ ë¶€í„° EOS tokenì´ ë‚˜ì˜¨ timestep $t=T$ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "Auto-regressive language generationì€ `GPT2`, `XLNet`, `OpenAi-GPT`, `CTRL`, `TransfoXL`, `XLM`, `Bart`, `T5` in both PyTorch and Tensorflow >= 2.0!ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë²ˆ ì‹¤ìŠµ ì‹œê°„ì—ëŠ” í˜„ì¬ ìœ ëª…í•œ decoding ë°©ë²•ë“¤ì¸ *Greedy search*, *Beam search*, *Top-K sampling*, *Top-p sampling* ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si4GyYhOQMzi"
   },
   "source": [
    "Transformerë¥¼ ì„¤ì¹˜í•˜ê³  Modelì„ load í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "ì´ë²ˆ ì‹¤ìŠµ ì‹œê°„ì—ì„œëŠ” SKTì—ì„œ ê³µê°œí•œ `KoGPT-2` ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (0.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from tokenizers) (0.24.6)\n",
      "Requirement already satisfied: filelock in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/piai/anaconda3/envs/torch/lib/python3.8/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843,
     "referenced_widgets": [
      "af43ff58e4ff4f31a67b6633d3646c63",
      "8ea439f12b8a42fea37c4398dc90de06",
      "2362996521a74c5c805a5d52ac147d25",
      "43a27a8bfaee47de8e3a257eae34d252",
      "ff9e42ecfda5435daea55188ff3a1b87",
      "94c9ae375fa84716bfcdb5a27a126fad",
      "71422f7bd7e84512aab6eb3cab9710a6",
      "b34f8ff388b34cc4b64442817726e1a3",
      "03827f7e1c154f7b9532db5fd5998474",
      "c21755e4c6f84d68a5bc5121aef53b91",
      "f94c44868d15456cb9cc1475c8af1008",
      "94b79b8928dc4388ba805f9b56964619",
      "97c1d6c674654bda9815f25accd6973f",
      "8a1370ae14d346b5b9b282ffbbb5b06a",
      "dbc6af604f4143bf9b668e488b3c277b",
      "8189695b14fe413586af9b623fd2fcd3",
      "6d873519f5c84524b7e6e211cbba35e7",
      "476a27e1cb424a18bb1aeaa41e1282c5",
      "13fdc5f36e42475a89b0b909fb713677",
      "7841f3e572ca44a782f2fb960a05d219",
      "5762ca0cd6704977ae7cb2233cd3e3d7",
      "daa9ebe319f44278b068933e92149b63",
      "ebfdf22d1bbc42ef8f75fa6d045062f2",
      "ebb4aa4d69584fc082a194e4c6c0a215",
      "6f84aa7b72cf42ec87416741cfdbd09b",
      "7d6e320d0fbe4fec86386815b2b048f8",
      "c050059abf4940a68cd0286b6abe51c0",
      "cb1843b434614a1cb34c5d202ce66edf",
      "c9da936c77a34620a1bfefbabe12da08",
      "3cf697b906d5435eb3b7bd4785875004",
      "0916bd6192854e099adf220ff7e9bd70",
      "a2942208a8514e22b9b73e8b7051dd7e",
      "d33bb694842f4eb0b6b83c0d0061af3d"
     ]
    },
    "executionInfo": {
     "elapsed": 27389,
     "status": "ok",
     "timestamp": 1719989066155,
     "user": {
      "displayName": "ê°•ì„í›ˆ",
      "userId": "02659803525042017137"
     },
     "user_tz": -540
    },
    "id": "ue2kOQhXTAMU",
    "outputId": "ffcc9374-c194-4b54-8607-44780e85b815",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/home/piai/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/piai/anaconda3/envs/torch/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m PreTrainedTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskt/kogpt2-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m   bos_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, eos_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, unk_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m   pad_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m, mask_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<mask>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskt/kogpt2-base-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/modeling_utils.py:2905\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2901\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2902\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2904\u001b[0m         )\n\u001b[0;32m-> 2905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.7 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8Y7cgu9ohXP"
   },
   "source": [
    "### **Greedy Search**\n",
    "\n",
    "Greedy searchëŠ” ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì„ íƒí•  ë•Œ, ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ë‹¨ì–´ë¥¼ ì„ íƒí•˜ëŠ” ë‹¨ìˆœí•œ ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "$w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$.\n",
    "\n",
    "ë‹¤ìŒ ê·¸ë¦¼ì€ greedy searchë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒ ì…ë‹ˆë‹¤.\n",
    "\n",
    "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
    "\n",
    "\n",
    "ì•Œê³ ë¦¬ì¦˜ì€ $\\text{\"The\"}$ì—ì„œ ì‹œì‘í•˜ì—¬ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ $\\text{\"nice\"}$ ë“±ì„ ì„ íƒí•˜ëŠ” íƒìš•ìŠ¤ëŸ¬ìš´(Greedy) ë°©ë²•ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ìµœì¢…ì ìœ¼ë¡œ ìƒì„± ëœ word sequenceëŠ” $\\text{\"The\", \"nice\", \"woman\"}$ì´ê³  ì „ì²´ í™•ë¥ ì€ $0.5 \\times 0.4 = 0.2$ ì…ë‹ˆë‹¤.\n",
    "\n",
    "`Transformers` greedy searchë¥¼ ì‚¬ìš©í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7046,
     "status": "ok",
     "timestamp": 1719989120725,
     "user": {
      "displayName": "ê°•ì„í›ˆ",
      "userId": "02659803525042017137"
     },
     "user_tz": -540
    },
    "id": "OWLd_J6lXz_t",
    "outputId": "4774c401-9b93-4f13-a326-4df18ddfe045"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizing\u001b[39m(text):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mì˜¤ëŠ˜ ì ì‹¬ì€\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# generate text until the output length (which includes the context length) reaches 100\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ìƒì„± ëª¨ë¸ì€ generate í•¨ìˆ˜ë¥¼ í†µí•´ ë‹¤ìŒ tokenì„ ìƒì„±í•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ê·¸ëƒ¥ ë„£ì–´ì£¼ë©´ ìë™ìœ¼ë¡œ greedy searchë¥¼ ì‹œì‘.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids)\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mtokenizing\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizing\u001b[39m(text):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "def tokenizing(text):\n",
    "    return torch.tensor(tokenizer.encode(text)).to('cuda').unsqueeze(0)\n",
    "\n",
    "input_ids = tokenizing(\"ì˜¤ëŠ˜ ì ì‹¬ì€\")\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 100\n",
    "# ìƒì„± ëª¨ë¸ì€ generate í•¨ìˆ˜ë¥¼ í†µí•´ ë‹¤ìŒ tokenì„ ìƒì„±í•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "# ê·¸ëƒ¥ ë„£ì–´ì£¼ë©´ ìë™ìœ¼ë¡œ greedy searchë¥¼ ì‹œì‘.\n",
    "print(input_ids)\n",
    "greedy_output = model.generate(input_ids, max_length=100,\n",
    "                            pad_token_id=tokenizer.pad_token_id,\n",
    "                            eos_token_id=tokenizer.eos_token_id,\n",
    "                            bos_token_id=tokenizer.bos_token_id,\n",
    "                            use_cache=True)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output.tolist()[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN7h9WiFBJ2p"
   },
   "source": [
    "GPT2ë¥¼ ì‚¬ìš©í•´ ì§§ì€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìƒì„±ëœ ë‹¨ì–´ë“¤ì˜ ë¬¸ë§¥ì€ í•©ë¦¬ì ì´ì§€ë§Œ, ëª¨ë¸ì´ ë°˜ë³µëœ ë‹¨ì–´ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŸ¬í•œ í˜„ìƒì€ ì¼ë°˜ì ì¸ ì–¸ì–´ ìƒì„± ëª¨ë¸ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê³µí†µëœ ë¬¸ì œì¸ë°, íŠ¹íˆ Greedy Searchì™€ Beam Searchì—ì„œ ê·¸ëŸ¬í•œ í˜„ìƒì´ ë”ìš± ë‘ë“œëŸ¬ì§€ê²Œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n",
    "\n",
    "Greedy Searchì˜ ì£¼ìš” ë‹¨ì ì€ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ë‚®ì€ í™•ë¥  ë‹¨ì–´ ì´í›„ì— ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ë” ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ë¥¼ ë†“ì¹œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´ ë‹¨ì–´ $\\text{has}$ëŠ” 0.9ë¡œ ë†’ì€ í™•ë¥ ì„ ê°–ì§€ë§Œ ì²«ë²ˆì§¸ ë‹¨ì–´ í›„ë³´ ì¤‘ ë‘ë²ˆì§¸ë¡œ ë†’ì€ conditional probabilityë¥¼ ê°–ëŠ” $\\text{dog}$ ì´í›„ì— ìˆ¨ì–´ ìˆëŠ” í˜•íƒœì…ë‹ˆë‹¤. ë”°ë¼ì„œ Greedy SearchëŠ” $\\text{\"The\"}, \\text{\"dog\"}, \\text{\"has\"}$ ë¼ëŠ” word sequenceë¥¼ ë†“ì¹˜ê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8DnXZ1WiuNd"
   },
   "source": [
    "### **Beam search**\n",
    "\n",
    "Beam searchëŠ” ê° time stepì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ hypothesisì˜ `num_beams`ë¥¼ ìœ ì§€í•˜ê³  ì „ì²´ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ hypothesisë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì¦‰, í•´ë‹¹ ì‹œì ì—ì„œ ìœ ë§í•œ ë¹”ì˜ ê°œìˆ˜ë§Œí¼(num_beams) ê³¨ë¼ì„œ ì§„í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë†’ì€ í™•ë¥ ì„ ê°€ì§€ê³  ìˆì§€ë§Œ ìˆ¨ì–´ìˆëŠ” word sequenceë¥¼ ë†“ì¹  ìœ„í—˜ì„ ì¤„ì…ë‹ˆë‹¤.ë‹¤ìŒ ê·¸ë¦¼ì€ `num_beams=2`ë¡œ ì„¤ì •í•œ Beam searchì˜ ì˜ˆì‹œì…ë‹ˆë‹¤:\n",
    "\n",
    "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "\n",
    "Time step $1$ : ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ hypothesisì¸ $\\text{\"The\", \"nice\"}$ ì™¸ì—ë„ beam searchëŠ” ë‘ë²ˆì§¸ë¡œ ê°€ëŠ¥ì„±ì´ ë†’ì€ $\\text{\"The\", \"dog\"}$ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.\n",
    "\n",
    "Time step $2$ : beam searchëŠ” $0.2$ì˜ ê°€ëŠ¥ì„±ì„ ê°€ì§„  $\\text{\"The\", \"nice\", \"woman\"}$ë³´ë‹¤ $0.36$ì˜ ê°€ëŠ¥ì„±ì„ ê°€ì§„ $\\text{\"The\", \"dog\", \"has\"}$ê°€ í™•ë¥ ì´ ë” ë†’ë‹¤ëŠ” ê²ƒì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë°©ë²•ì€ ìš°ë¦¬ì˜ toy exampleì—ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ word sequenceë¥¼ ì°¾ì•„ëƒˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Beam searchëŠ” í•­ìƒ Greedy searchë³´ë‹¤ ë†’ì€ í™•ë¥ ì˜ ê²°ê³¼ sequenceë¥¼ ì°¾ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì´ê²Œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²°ê³¼ë¼ê³ ëŠ” ë³´ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
    "\n",
    "`transformers`ì—ì„œ beam searchë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ ë´…ì‹œë‹¤. ìš°ë¦¬ëŠ” `num_beams > 1`, `early_stopping=True` ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  beam hypothesisê°€ eosí† í°ì— ë‹¿ìœ¼ë©´ ìƒì„±ì„ ë§ˆì¹˜ë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2085,
     "status": "ok",
     "timestamp": 1719989240519,
     "user": {
      "displayName": "ê°•ì„í›ˆ",
      "userId": "02659803525042017137"
     },
     "user_tz": -540
    },
    "id": "R1R5kx30Ynej",
    "outputId": "75e904f2-ea43-4279-990a-48909dea68db"
   },
   "outputs": [],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yi0sRvZwGHos"
   },
   "source": [
    "ê²°ê³¼ëŠ” ë” ìœ ì°½í•˜ê²Œ ë³´ì´ì§€ë§Œ ì—¬ì „íˆ ë™ì¼í•œ word sequenceë¥¼ ë°˜ë³µí•˜ëŠ” ë¬¸ì œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¨ìˆœí•œ í•´ê²°ë²•ì€ [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304)ì™€ [Klein et al. (2017)](https://arxiv.org/abs/1701.02810)ì˜ ë…¼ë¬¸ì—ì„œ ì œì•ˆ ëœ n-grams í˜ë„í‹°ë¥¼ ë„ì…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°€ì¥ ì¼ë°˜ì ì¸ n-grams í˜ë„í‹°ëŠ” ì´ë¯¸ ë‚˜íƒ€ë‚œ n-gramì— ëŒ€í•´ ë‹¤ìŒ ë‹¨ì–´ë¡œ ìƒì„± ë  í™•ë¥ ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë‘ë²ˆ ë‚˜íƒ€ë‚˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "`no_repeat_ngram_size=2`ë¥¼ ì„¤ì •í•´ 2-gramì´ ë‘ë²ˆ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ ë§‰ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1719989306305,
     "user": {
      "displayName": "ê°•ì„í›ˆ",
      "userId": "02659803525042017137"
     },
     "user_tz": -540
    },
    "id": "jy3iVJgfnkMi",
    "outputId": "eabf0392-8df1-406e-d31e-1d011629b256"
   },
   "outputs": [],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=2,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxsksOGDpmA0"
   },
   "source": [
    "ë”ì´ìƒ ë°˜ë³µì´ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ, n-gram í˜ë„í‹°ëŠ” ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, city New Yorkì— ëŒ€í•´ ìƒì„±ëœ ê¸°ì‚¬ëŠ” n-gramì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. 2-gramì„ ì‚¬ìš©í•˜ê²Œ ë  ê²½ìš° ì‹œì˜ ì´ë¦„ì´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í•œ ë²ˆë§Œ ë‚˜íƒ€ë‚˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "Beam searchì˜ ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ íŠ¹ì§•ì€ ìƒì„±ëœ Top beamì„ ë¹„êµí•˜ì—¬ ëª©ì ì— ê°€ì¥ ì í•©í•œ Beamì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "`Transformers`ì—ì„œ ìš°ë¦¬ëŠ” `num_return_sequences`ë¥¼ top-nê°œì˜ ë†’ì€ scoringì„ ê°€ì§„ beamì„ returní•  ê²ƒì¸ì§€ ì„¤ì • í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë‹¨, num_return_sequencesëŠ” num_beamsë³´ë‹¤ ê°™ê±°ë‚˜ ì‘ì•„ì•¼í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2097,
     "status": "ok",
     "timestamp": 1719989372359,
     "user": {
      "displayName": "ê°•ì„í›ˆ",
      "userId": "02659803525042017137"
     },
     "user_tz": -540
    },
    "id": "5ClO3VphqGp6",
    "outputId": "61a826c0-539a-48ee-ab6c-2ace5963cb78"
   },
   "outputs": [],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences = 5,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output.tolist(), skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhLKyfdbsjXc"
   },
   "source": [
    "ìœ„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ Top 5ì˜ Beam hypothesisëŠ” ì„œë¡œ ì•½ê°„ë§Œ ë‹¤ë¥¼ ë¿ì´ë©° 5ê°œë§Œ ì‚¬ìš©í–ˆì„ ê²½ìš° ë³„ë¡œ ë†€ë„ë§Œí•œ ê²°ê³¼ëŠ” ì•„ë‹™ë‹ˆë‹¤.\n",
    "\n",
    "Open-ended generationì—ì„œ, beam searchê°€ ìµœì„ ì˜ ì„ íƒì´ ì•„ë‹ ìˆ˜ ìˆëŠ” ëª‡ê°€ì§€ ì´ìœ ê°€ ì œì‹œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- Beam searchëŠ” Machine translation ë˜ëŠ” Text summarizationì²˜ëŸ¼ ì›í•˜ëŠ” ë¬¸ì¥ ìƒì„± ê¸¸ì´ê°€ ì˜ˆì¸¡ ê°€ëŠ¥í•œ taskì—ì„œëŠ” ì˜ ì‘ë™ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ Dialog ë˜ëŠ” Story generation taskì²˜ëŸ¼ ì¶œë ¥ ê¸¸ì´ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆëŠ” open-ended generationì—ì„œëŠ” ì›í™œí•˜ê²Œ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ - see [Murray et al. (2018)](https://arxiv.org/abs/1808.10006) and [Yang et al. (2018)](https://arxiv.org/abs/1808.09582).\n",
    "\n",
    "- Beam searchëŠ” ë°˜ë³µ ìƒì„± ë¬¸ì œì— ì·¨ì•½í•©ë‹ˆë‹¤. íŠ¹íˆ Story generation taskì—ì„œ n-gram ë˜ëŠ” ê¸°íƒ€ í˜ë„í‹°ë¥¼ í†µí•´ ë¬¸ì¥ì„ ì œì–´í•˜ëŠ” ê²ƒì´ ì–´ë µìŠµë‹ˆë‹¤. *ë°˜ë³µì´ ì—†ëŠ” êµ¬ë¬¸* ê³¼ *n-gram*ì˜ ë°˜ë³µ ì£¼ê¸° ì‚¬ì´ì—ì„œ ì ë‹¹í•œ trade-offë¥¼ ì°¾ê¸° ìœ„í•´ ë§ì€ fine-tuningì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "- [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)ëŠ” ê³ í’ˆì§ˆ ì¸ê°„ ì–¸ì–´ëŠ” ë†’ì€ í™•ë¥ ì˜ ë‹¤ìŒ ë‹¨ì–´ ë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤. ì‰½ê²Œ ë§í•˜ìë©´ ì¸ê°„ ì…ì¥ì—ì„œ ìš°ë¦¬ëŠ” ì§€ë£¨í•˜ê±°ë‚˜ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¬¸ì¥ì´ ì•„ë‹ˆë¼ ìš°ë¦¬ë¥¼ ë†€ë¼ê²Œ í•  ìˆ˜ ìˆëŠ” ë¬¸ì¥ ìƒì„±ì„ ì›í•œë‹¤ê³  í•©ë‹ˆë‹¤. ì €ìëŠ” ëª¨ë¸ì´ human text vs. beam seachë¥¼ graphë¡œ ë³´ì—¬ì£¼ë©´ì„œ beam search textê°€ ê·¸ë‹¤ì§€ ë†€ëì§€ ì•Šì€ ë¬¸ì¥ì„ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n",
    "\n",
    "\n",
    "So let's stop being boring and introduce some randomness ğŸ¤ª."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbbIyK84wHq6"
   },
   "source": [
    "### **Sampling**\n",
    "\n",
    "ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ samplingì€ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ì–´ $w_t$ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "ì•„ë˜ ì‚¬ì§„ì€ sampling í•  ë•Œ, ì–¸ì–´ ìƒì„±ì„ ì‹œê°í™”í•œ í˜•íƒœì…ë‹ˆë‹¤.\n",
    "\n",
    "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
    "\n",
    "Samplingì„ ì´ìš©í•œ ì–¸ì–´ ìƒì„±ì€ ë”ì´ìƒ *deterministic*í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë‹¨ì–´ $\\text{\"car\"}$ ëŠ” contional probability distribution $P(w | \\text{\"The\"})$ì—ì„œ ìƒ˜í”Œë§ ë˜ê³ , $P(w | \\text{\"The\"}, \\text{\"car\"})$ëŠ” $\\text{\"drives\"}$ë¥¼ ìƒ˜í”Œë§ í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "`Transformers`ì—ì„œ ìš°ë¦¬ëŠ” `do_sample=True` ë¥¼ ì„¤ì •í•˜ê³  `top_k=0`ë¡œ ë‘ì–´ *Top-K* ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.(ë’¤ì—ì„œ ë‹¤ë£° ê²ƒ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1477,
     "status": "ok",
     "timestamp": 1719989559617,
     "user": {
      "displayName": "ê°•ì„í›ˆ",
      "userId": "02659803525042017137"
     },
     "user_tz": -540
    },
    "id": "aRAz4D-Ks0_4",
    "outputId": "fe1e5c6b-9291-490b-899b-dd852c19ee6a"
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(53) #ì›í•œë‹¤ë©´ random seedë¥¼ ì§€ì • í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "\n",
    "\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "croVNefgNC9F"
   },
   "source": [
    "ê´œì°®ì•„ ë³´ì´ì§€ë§Œ ì¼ê´€ì„±ì´ ì—†ìŠµë‹ˆë‹¤. ì´ê²ƒì€ sampling word sequencesë¥¼ í• ë•Œ ëª¨ë¸ì´ ì¼ê´€ì„±ì—†ì´ íš¡ì„¤ìˆ˜ì„¤í•˜ëŠ” ë¬¸ì¥ì„ ë°œìƒì‹œí‚¤ëŠ” í° ë¬¸ì œì…ë‹ˆë‹¤. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)).\n",
    "ëª¨ë¸ì´ ë§Œë“¤ì–´ ë‚¸ í™•ë¥ ì´ smoothí•œ ë‚˜ë¨¸ì§€, ë‚®ì€ í™•ë¥ ì˜ í† í°ì´ ì§€ë‚˜ì¹˜ê²Œ ì˜ ìƒ˜í”Œë§ ë˜ëŠ” ê²ƒì´ ì›ì¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "í•œê°€ì§€ íŠ¸ë¦­ì€ [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max) ì˜ ì´ë¥¸ë°” `temperature`ë¥¼ ë‚®ì¶”ì–´ ë¶„í¬ $P(w|w_{1:t-1})$ë¥¼ ë” ì„ ëª…í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ì˜ ê°€ëŠ¥ì„±ì€ ì¦ê°€ì‹œí‚¤ê³  ë‚®ì€ í™•ë¥ ì˜ ë‹¨ì–´ ê°€ëŠ¥ì„±ì€ ê°ì†Œì‹œí‚¤ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "temperatureê°€ 0ì— ê°€ê¹Œì›Œì§ˆ ìˆ˜ë¡ greedy decodingì— ê°€ê¹Œìš´ ì•„ì›ƒí’‹ì´ ë‚˜ì˜µë‹ˆë‹¤.\n",
    "\n",
    "temperatureë¥¼ ì ìš©í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê·¸ë¦¼ì„ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
    "\n",
    "step=1ì˜ ë‹¤ìŒ ë‹¨ì–´ ë¶„í¬ëŠ” ë”ìš± ì„ ëª…í•´ì¡Œê¸° ë•Œë¬¸ì— ë‹¨ì–´ $\\text{\"car\"}$ë¥¼ ì„ íƒí•  í™•ë¥ ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "`temperature=0.7`ë¥¼ ì„¤ì •í•˜ì—¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë¶„í¬ë¥¼ ì–´ë–»ê²Œ ë³€í™”ì‹œí‚¤ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1352,
     "status": "ok",
     "timestamp": 1719989657142,
     "user": {
      "displayName": "ê°•ì„í›ˆ",
      "userId": "02659803525042017137"
     },
     "user_tz": -540
    },
    "id": "zIBRDVLSOmpC",
    "outputId": "d0bc9126-2c69-4283-f1b5-a149df743885"
   },
   "outputs": [],
   "source": [
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature = 0.7,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsjsAeI3Ot2w"
   },
   "source": [
    "ì´ì œ ì´ìƒí•œ n-gramì´ ì ê³  ì¶œë ¥ ë¬¸ì¥ì´ ì¡°ê¸ˆ ë” ì¼ê´€ì„± ìˆê²Œ ìƒì„±ë©ë‹ˆë‹¤. temperatureë¥¼ ì ìš©í•˜ë©´ ë¶„í¬ê°€ ëœ randomí•˜ì§€ë§Œ `temperature` $ \\to 0$ë¡œ ì„¤ì •í•œë‹¤ë©´ temperatureê°€ ì ìš©ëœ samplingì€ greedy decodingê³¼ ê°™ì•„ì§€ë©° ì´ì „ê³¼ ë™ì¼í•œ ë¬¸ì œë¥¼ ê²ªê²Œ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "binNTroyzQBu"
   },
   "source": [
    "### **Top-K Sampling**\n",
    "\n",
    "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) ì€ ê°„ë‹¨í•˜ì§€ë§Œ ë§¤ìš° ê°•ë ¥í•œ ìƒ˜í”Œë§ ë°©ì‹ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. . *Top-K* samplingì—ì„œ ë†’ì€ ê°€ëŠ¥ì„±ì„ ê°€ì§„ kê°œë¥¼ ì œì™¸í•œ ë‹¨ì–´ëŠ” í•„í„°ë§ ë˜ê³  k ì´í›„ì˜ probablity massëŠ” ì¬ë¶„ë°°ë©ë‹ˆë‹¤. GPT2ëŠ” Top-K Samplingë°©ì‹ì„ ì±„íƒí–ˆëŠ”ë°, ì´ê²ƒì´ Story Gerneration Taskì— ì„±ê³µí•œ ì´ìœ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "Top-K Samplingì„ ë” ì˜ ì„¤ëª…í•˜ê¸° ìœ„í•´ ìœ„ì˜ ì˜ˆì œì—ì„œ ë‘ Sampling stepì— ì‚¬ìš©ë˜ëŠ” ë²”ìœ„ë¥¼ 3ë‹¨ì–´ì—ì„œ 10ë‹¨ì–´ë¡œ í™•ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
    "\n",
    "\n",
    "K=6ì„ ì„¤ì •í•˜ë©´ ë‘ Sampling stepsì—ì„œ Sampling poolì„ 6ê°œì˜ ë‹¨ì–´ë¡œ ì œí•œí•©ë‹ˆë‹¤.\n",
    "\n",
    "$\\text{\"The\"}$ ë‹¤ìŒìœ¼ë¡œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” 6ê°œë¥¼ ì„ íƒí•˜ê³  $\\text{\"The:, \"car\"}$ ë’¤ì— ì˜¬ ìˆ˜ ìˆëŠ” 6ê°œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì²« stepì—ì„œ ì „ì²´ í™•ë¥  ì§ˆëŸ‰ì˜ 2/3ì¸ 0.68ì •ë„ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ì—ì„œ ë””ì½”ë”©ë˜ì§€ë§Œ, ë‘ë²ˆì§¸ stepì—ì„œ ê±°ì˜ ëª¨ë“  í™•ë¥ ì§ˆëŸ‰ì¸ 0.99ì—ì„œ ë””ì½”ë”©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ê·¸ê²ƒì´ ë‘ë²ˆì§¸ sampling stepì—ì„œ $\\text{\"not\", \"the\", \"small\", \"told\"}$ ì™€ ê°™ì€ ë‹¤ì†Œ ì´ìƒí•œ í›„ë³´ë“¤ì„ ì„±ê³µì ìœ¼ë¡œ ì œê±°ê°€ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "200tV_DPQCvn"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1227,
     "status": "ok",
     "timestamp": 1719989753847,
     "user": {
      "displayName": "ê°•ì„í›ˆ",
      "userId": "02659803525042017137"
     },
     "user_tz": -540
    },
    "id": "HBtDOdD0wx3l",
    "outputId": "caed674b-2fe2-418e-d87e-c565339bade0"
   },
   "outputs": [],
   "source": [
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature = 0.7,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y77H5m4ZmhEX"
   },
   "source": [
    "ì§€ê¸ˆê¹Œì§€ ë´ì˜¨ decoding methods ì¤‘ ê°€ì¥ ì‚¬ëŒë‹¤ì›Œ ë³´ì´ëŠ” í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. Top-K Samplingì˜ í•œê°€ì§€ ìš°ë ¤ë˜ëŠ” ì ì€ ë‹¤ìŒ ë‹¨ì–´ í™•ë¥  ë¶„í¬ $P(w|w_{1:t-1})$ì—ì„œ í•„í„°ë§ ëœ ë‹¨ì–´ ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ì§€ ì•ŠëŠ” ì ì…ë‹ˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ ìœ„ ê·¸ë¦¼ì—ì„œ ì²«ë²ˆì§¸ stepì˜ ë‹¨ì–´ë“¤ì€ ì „ë°˜ì ìœ¼ë¡œ í‰í‰í•œ ë¶„í¬ì—ì„œ samplingë˜ì§€ë§Œ, ë‘ë²ˆì§¸ stepì˜ ì–´ë–¤ ë‹¨ì–´ë“¤ì€ ë§¤ìš° sharpí•œ ë¶„í¬ì—ì„œ sampling ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Step $t=1$ì—ì„œ Top-Kì€ ê½¤ í•©ë¦¬ì ì¸ í›„ë³´ì²˜ëŸ¼ ë³´ì´ëŠ” $\\text{\"people\", \"big\", \"house\", \"cat\"}$ì„ ìƒ˜í”Œë§í•˜ëŠ” ê°€ëŠ¥ì„±ì„ ë°°ì œí•©ë‹ˆë‹¤. ë°˜ë©´ì— Step $t=2$ì—ì„œ ë‹¨ì–´ Sample poolì— ë‹¨ì–´ $\\text{\"down\", \"a\"}$ì™€ ê°™ì€ ë¶€ì ì ˆí•œ ë‹¨ì–´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ Sample poolì´ ê³ ì •í¬ê¸° Kë¡œ ì œí•œë˜ë©´ ëª¨í˜•ì´ Sharpí•œ ë¶„í¬ì—ì„œ íš¡ì„¤ìˆ˜ì„¤í•œ ë‹¨ì–´ë¥¼ ê³ ë¥¼ ìœ„í—˜ì´ìˆê³  í‰í‰í•œ ë¶„í¬ì—ì„œëŠ” ë¬¸ì¥ì˜ ì°½ì˜ì„±ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki9LAaexzV3H"
   },
   "source": [
    "### **Top-p (nucleus) sampling**\n",
    "\n",
    "*Top-p* samplingì€ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ Kê°œì—ì„œë§Œ sampleì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ëˆ„ì  í™•ë¥ ì´ í™•ë¥  pë¥¼ ì´ˆê³¼í•˜ëŠ” ìµœì†Œí•œì˜ ë‹¨ì–´ ì§‘í•©ì—ì„œ sampleì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ í›„ í™•ë¥  ì§ˆëŸ‰ì´ ë‹¨ì–´ ì§‘í•© ì‚¬ì´ì— ì¬ë¶„ë°° ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¤ìŒ ë‹¨ì–´ì˜ í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ê°€ ë™ì ìœ¼ë¡œ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
    "\n",
    "$p=0.92$ë¡œ ì„¤ì •í•  ê²½ìš°, *Top-p* ëŠ” $p=92\\%$ë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ë‹¨ì–´ ìˆ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ ìŠ¤í…ì—ì„œ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ë‹¨ì–´ 9ê°œê°€ í¬í•¨ëœ ë°˜ë©´, ë‘ë²ˆì§¸ ìŠ¤í…ì—ì„œëŠ” top 3ê°œë§Œ ì„ íƒí•´ë„ $p=92\\%$ë¥¼ ì´ˆê³¼í•˜ê²Œ ë©ë‹ˆë‹¤. ì¦‰, ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ì—ë§Œ samplingì„ í•˜ê³  ê·¸ë ‡ì§€ ì•Šì€ ë‹¨ì–´ëŠ” samplingí•  í™•ë¥ ì´ ë§¤ìš° ì ì–´ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "`Transformers`ì—ì„œ `top_p âˆˆ (0,1)`ì„ ì„¤ì •í•˜ì—¬ *Top-p* samplingì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1772,
     "status": "ok",
     "timestamp": 1719989816216,
     "user": {
      "displayName": "ê°•ì„í›ˆ",
      "userId": "02659803525042017137"
     },
     "user_tz": -540
    },
    "id": "EvwIc7YAx77F",
    "outputId": "e691ba02-e2c2-49df-c04d-a558c4158db0"
   },
   "outputs": [],
   "source": [
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    top_p = 0.92,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn-8gLaR4lat"
   },
   "source": [
    "ì´ë¡ ì ìœ¼ë¡œëŠ” *Top-p*ê°€ *Top-K*ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•„ ë³´ì´ì§€ë§Œ, ë‘ ë°©ë²• ëª¨ë‘ ì‹¤ì œë¡œ ì˜ ì‘ë™í•©ë‹ˆë‹¤.\n",
    "\n",
    "*Top-p*ì™€ *Top-K*ëŠ” í•¨ê»˜ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ë§¤ìš° ë‚®ì€ ìˆœìœ„ì˜ ë‹¨ì–´ë¥¼ í”¼í•˜ë©´ì„œë„ ì¼ë¶€ ë™ì  ì„ íƒì„ í—ˆìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë…ë¦½ì ìœ¼ë¡œ samplingëœ multiple outputsë¥¼ ì–»ê¸° ìœ„í•´ `num_return_sequences > 1`ë¡œ ì„¤ì •í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1958,
     "status": "ok",
     "timestamp": 1719989863267,
     "user": {
      "displayName": "ê°•ì„í›ˆ",
      "userId": "02659803525042017137"
     },
     "user_tz": -540
    },
    "id": "3kY8P9VG8Gi9",
    "outputId": "e9a2b7bc-45fe-49a2-c616-3f6eedb0a016"
   },
   "outputs": [],
   "source": [
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p = 0.92,\n",
    "    num_return_sequences = 5,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output.tolist(), skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vRPfMl88rk0"
   },
   "source": [
    "Cool, now you should have all the tools to let your model write your stories with `transformers`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsWd7e98Vcs3"
   },
   "source": [
    "### **Conclusion**\n",
    "\n",
    "*ad-hoc* decoding methodsì— ë”°ë¥´ë©´ open-ended generationì—ì„œ *top-p* and *top-K*ëŠ” *greedy*, *beam search*ë³´ë‹¤ ë”ìš± ìœ ì°½í•œ textë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ  [Welleck et al. (2020)](https://arxiv.org/abs/2002.02492)ì— ë”°ë¥´ë©´ *top-K*ì™€ *top-p*ëŠ” ì—¬ì „íˆ ë°˜ë³µë˜ëŠ” word sequencesë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œ(*greedy*, *beam search*ì™€ ê°™ì´)ë¥¼ ê²ªê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤.\n",
    "\n",
    "[Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf)ëŠ” human evalutation ê´€ì ì—ì„œ, model training ëª©ì  í•¨ìˆ˜ë¥¼ ì˜ ì¡°ì •í•˜ë©´, *beam search*ê°€ *Top-p*ë³´ë‹¤ ìœ ì°½í•œ textë¥¼ ìƒì„±í•œë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "Open-ended language generationì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ëŠ” ë¶„ì•¼ì´ë©°, ë¬´ì—‡ì´ ì í•©í•˜ë‹¤ê³  ë‹¨ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì—ì„œ ê°€ì¥ ì˜ ì‘ë™í•˜ëŠ” ë°©ë²•ì´ ë¬´ì—‡ì¸ì§€ ê³ ë ¤í•´ì•¼í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4CYi91h11yd"
   },
   "source": [
    "### **Appendix**\n",
    "\n",
    "There are a couple of additional parameters for the `generate` method that were not mentioned above. We will explain them here briefly!\n",
    "\n",
    "- `min_length` can be used to force the model to not produce an EOS token (= not finish the sentence) before `min_length` is reached. This is used quite frequently in summarization, but can be useful in general if the user wants to have longer outputs.\n",
    "- `repetition_penalty` can be used to penalize words that were already generated or belong to the context. It was first introduced by [Kesker et al. (2019)](https://arxiv.org/abs/1909.05858) and is also used in the training objective in [Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf). It can be quite effective at preventing repetitions, but seems to be very sensitive to different models and use cases, *e.g.* see this [discussion](https://github.com/huggingface/transformers/pull/2303) on Github.\n",
    "\n",
    "- `attention_mask` can be used to mask padded tokens\n",
    "- `pad_token_id`, `bos_token_id`, `eos_token_id`: If the model does not have those tokens by default, the user can manually choose other token ids to represent them.\n",
    "\n",
    "For more information please also look into the `generate` function [docstring](https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03827f7e1c154f7b9532db5fd5998474": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0916bd6192854e099adf220ff7e9bd70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "13fdc5f36e42475a89b0b909fb713677": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2362996521a74c5c805a5d52ac147d25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b34f8ff388b34cc4b64442817726e1a3",
      "max": 2825034,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_03827f7e1c154f7b9532db5fd5998474",
      "value": 2825034
     }
    },
    "3cf697b906d5435eb3b7bd4785875004": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43a27a8bfaee47de8e3a257eae34d252": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c21755e4c6f84d68a5bc5121aef53b91",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f94c44868d15456cb9cc1475c8af1008",
      "value": "â€‡2.83M/2.83Mâ€‡[00:00&lt;00:00,â€‡6.64MB/s]"
     }
    },
    "476a27e1cb424a18bb1aeaa41e1282c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5762ca0cd6704977ae7cb2233cd3e3d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d873519f5c84524b7e6e211cbba35e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f84aa7b72cf42ec87416741cfdbd09b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cf697b906d5435eb3b7bd4785875004",
      "max": 513302779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0916bd6192854e099adf220ff7e9bd70",
      "value": 513302779
     }
    },
    "71422f7bd7e84512aab6eb3cab9710a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7841f3e572ca44a782f2fb960a05d219": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7d6e320d0fbe4fec86386815b2b048f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2942208a8514e22b9b73e8b7051dd7e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d33bb694842f4eb0b6b83c0d0061af3d",
      "value": "â€‡513M/513Mâ€‡[00:07&lt;00:00,â€‡58.1MB/s]"
     }
    },
    "8189695b14fe413586af9b623fd2fcd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a1370ae14d346b5b9b282ffbbb5b06a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13fdc5f36e42475a89b0b909fb713677",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7841f3e572ca44a782f2fb960a05d219",
      "value": 1000
     }
    },
    "8ea439f12b8a42fea37c4398dc90de06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94c9ae375fa84716bfcdb5a27a126fad",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_71422f7bd7e84512aab6eb3cab9710a6",
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "94b79b8928dc4388ba805f9b56964619": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_97c1d6c674654bda9815f25accd6973f",
       "IPY_MODEL_8a1370ae14d346b5b9b282ffbbb5b06a",
       "IPY_MODEL_dbc6af604f4143bf9b668e488b3c277b"
      ],
      "layout": "IPY_MODEL_8189695b14fe413586af9b623fd2fcd3"
     }
    },
    "94c9ae375fa84716bfcdb5a27a126fad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97c1d6c674654bda9815f25accd6973f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d873519f5c84524b7e6e211cbba35e7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_476a27e1cb424a18bb1aeaa41e1282c5",
      "value": "config.json:â€‡100%"
     }
    },
    "a2942208a8514e22b9b73e8b7051dd7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af43ff58e4ff4f31a67b6633d3646c63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ea439f12b8a42fea37c4398dc90de06",
       "IPY_MODEL_2362996521a74c5c805a5d52ac147d25",
       "IPY_MODEL_43a27a8bfaee47de8e3a257eae34d252"
      ],
      "layout": "IPY_MODEL_ff9e42ecfda5435daea55188ff3a1b87"
     }
    },
    "b34f8ff388b34cc4b64442817726e1a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c050059abf4940a68cd0286b6abe51c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c21755e4c6f84d68a5bc5121aef53b91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9da936c77a34620a1bfefbabe12da08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb1843b434614a1cb34c5d202ce66edf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d33bb694842f4eb0b6b83c0d0061af3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "daa9ebe319f44278b068933e92149b63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dbc6af604f4143bf9b668e488b3c277b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5762ca0cd6704977ae7cb2233cd3e3d7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_daa9ebe319f44278b068933e92149b63",
      "value": "â€‡1.00k/1.00kâ€‡[00:00&lt;00:00,â€‡20.1kB/s]"
     }
    },
    "ebb4aa4d69584fc082a194e4c6c0a215": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb1843b434614a1cb34c5d202ce66edf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c9da936c77a34620a1bfefbabe12da08",
      "value": "pytorch_model.bin:â€‡100%"
     }
    },
    "ebfdf22d1bbc42ef8f75fa6d045062f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebb4aa4d69584fc082a194e4c6c0a215",
       "IPY_MODEL_6f84aa7b72cf42ec87416741cfdbd09b",
       "IPY_MODEL_7d6e320d0fbe4fec86386815b2b048f8"
      ],
      "layout": "IPY_MODEL_c050059abf4940a68cd0286b6abe51c0"
     }
    },
    "f94c44868d15456cb9cc1475c8af1008": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff9e42ecfda5435daea55188ff3a1b87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

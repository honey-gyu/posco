{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d4f470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: absl-py in /home/piai/anaconda3/lib/python3.10/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: click in /home/piai/anaconda3/lib/python3.10/site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in /home/piai/anaconda3/lib/python3.10/site-packages (from tensorflow_datasets) (1.23.5)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /home/piai/anaconda3/lib/python3.10/site-packages (from tensorflow_datasets) (4.25.4)\n",
      "Requirement already satisfied: psutil in /home/piai/anaconda3/lib/python3.10/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Collecting pyarrow (from tensorflow_datasets)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/piai/anaconda3/lib/python3.10/site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Collecting simple-parsing (from tensorflow_datasets)\n",
      "  Downloading simple_parsing-0.1.5-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in /home/piai/anaconda3/lib/python3.10/site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm in /home/piai/anaconda3/lib/python3.10/site-packages (from tensorflow_datasets) (4.66.4)\n",
      "Requirement already satisfied: wrapt in /home/piai/anaconda3/lib/python3.10/site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Collecting array-record>=0.5.0 (from tensorflow_datasets)\n",
      "  Downloading array_record-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (699 bytes)\n",
      "Collecting etils>=1.6.0 (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets)\n",
      "  Downloading etils-1.9.4-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: fsspec in /home/piai/anaconda3/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (2024.3.1)\n",
      "Collecting importlib_resources (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets)\n",
      "  Downloading importlib_resources-6.4.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/piai/anaconda3/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (4.11.0)\n",
      "Collecting zipp (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets)\n",
      "  Downloading zipp-3.20.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/piai/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/piai/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/piai/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/piai/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.7.4)\n",
      "Requirement already satisfied: six in /home/piai/anaconda3/lib/python3.10/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting docstring-parser~=0.15 (from simple-parsing->tensorflow_datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting protobuf>=3.20 (from tensorflow_datasets)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Downloading tensorflow_datasets-4.9.6-py3-none-any.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.9.4-py3-none-any.whl (164 kB)\n",
      "Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.5-py3-none-any.whl (113 kB)\n",
      "Downloading tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
      "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading importlib_resources-6.4.4-py3-none-any.whl (35 kB)\n",
      "Downloading zipp-3.20.1-py3-none-any.whl (9.0 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=a222a54ba7474829708ca6df0ff691e7befb3c51615b3f1252a276eef9231c99\n",
      "  Stored in directory: /home/piai/.cache/pip/wheels/54/4e/28/3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, zipp, toml, pyarrow, protobuf, promise, importlib_resources, immutabledict, etils, docstring-parser, tensorflow-metadata, simple-parsing, array-record, tensorflow_datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.4\n",
      "    Uninstalling protobuf-4.25.4:\n",
      "      Successfully uninstalled protobuf-4.25.4\n",
      "Successfully installed array-record-0.5.1 dm-tree-0.1.8 docstring-parser-0.16 etils-1.9.4 immutabledict-4.2.0 importlib_resources-6.4.4 promise-2.3 protobuf-3.20.3 pyarrow-17.0.0 simple-parsing-0.1.5 tensorflow-metadata-1.15.0 tensorflow_datasets-4.9.6 toml-0.10.2 zipp-3.20.1\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e701eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 15:25:07.496339: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-05 15:25:07.497723: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-05 15:25:07.501712: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-05 15:25:07.515031: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-05 15:25:07.537477: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-05 15:25:07.544012: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-05 15:25:07.560020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-05 15:25:08.560644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import Model, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29249ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 15:26:05.596305: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /home/piai/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bada670567e403b8a924329a4fba601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /home/piai/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = tfds.load('mnist', split = 'train')\n",
    "batch_size = 1024\n",
    "train_data = dataset.map(lambda data : tf.cast(data['image'], tf.float32) / 255.).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78766c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.generator = tf.keras.Sequential([layers.Dense(7 * 7 * 32, activation = 'relu'),\n",
    "                                             layers.Reshape((7, 7, 32)),\n",
    "                                             layers.Conv2DTranspose(64, 3, strides = 2, padding = 'same', activation = 'relu'),\n",
    "                                             layers.Conv2DTranspose(32, 3, strides = 2, padding = 'same', activation = 'relu'),\n",
    "                                             layers.Conv2DTranspose(1, 3, strides = 1, padding = 'same', activation = 'sigmoid')])\n",
    "    def call(self, z):\n",
    "        return self.generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a6bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "            layers.Conv2D(32, 3, strides = 2, activation = 'relu', padding = 'same', input_shape = (28, 28, 1)),\n",
    "            layers.Conv2D(64, 3, strides = 2, activation = 'relu', padding = 'same'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "    def call(self, x):\n",
    "        return self.discriminator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d728f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "latent_dim = 10\n",
    "log_interval = 20\n",
    "\n",
    "generator = Generator(latent_dim)\n",
    "discriminator = Discriminator(latent_dim)\n",
    "\n",
    "optimizer_g = tf.keras.optimizers.Adam(1e-3)\n",
    "optimizer_d = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan에서 training / loss function 정의 부터 다시 해야함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
